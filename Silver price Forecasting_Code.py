# -*- coding: utf-8 -*-
"""Skripsi Dylan Norbert Gono 140110190031

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tYVyMXKCA0ovyi-3qGFlXIkZK1TCv7dL
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import xgboost as xgb
from sklearn.model_selection import KFold
from xgboost import XGBRegressor
from xgboost import plot_tree

color_pal = sns.color_palette()

"""# **Data Importing**"""

# reading the dataset using read_csv
df_silver = pd.read_csv("/content/drive/MyDrive/SKRIPSI/Silver Futures Historical Data.csv",
                 parse_dates=True,
                 index_col="Date")

df_gold = pd.read_csv("/content/drive/MyDrive/SKRIPSI/Gold Futures Historical Data.csv",
                 parse_dates=True,
                 index_col="Date")

df_plat = pd.read_csv("/content/drive/MyDrive/SKRIPSI/Platinum Futures Historical Data.csv",
                 parse_dates=True,
                 index_col="Date")

df_dol = pd.read_csv("/content/drive/MyDrive/SKRIPSI/USD_EUR Historical Data.csv",
                 parse_dates=True,
                 index_col="Date")

# take only column date and price
df_silver = df_silver[['Price']].rename(columns={"Price": "Silver Price"})
df_gold = df_gold[['Price']].rename(columns={"Price": "Gold Price"})
df_plat = df_plat[['Price']].rename(columns={"Price": "Platinum Price"})
df_dol = df_dol[['Price']].rename(columns={"Price": "USD/EUR"})

# displaying the first five rows of dataset
df_silver.tail()

# slicing data
slice_date_from = '2013-02-20'
slice_date_to = '2023-02-20'

df_ssilver = df_silver[slice_date_from : slice_date_to]
df_sgold = df_gold[slice_date_from : slice_date_to]
df_splat = df_plat[slice_date_from : slice_date_to]
df_sdol = df_dol[slice_date_from : slice_date_to]

# plotting data to graph
df_ssilver.plot(style='.', figsize=(20, 5), color=color_pal[0], title='Silver Future for Last 10 Years')
df_sgold.plot(style='.', figsize=(20, 5), color=color_pal[1], title='Gold Future for Last 10 Years')
df_splat.plot(style='.', figsize=(20, 5), color=color_pal[2], title='Platinum Future for Last 10 Years')
df_sdol.plot(style='.', figsize=(20, 5), color=color_pal[3], title='USD/EUR for Last 10 Years')

# merge data into one dataframe (after shifting)
df_merged = pd.merge(df_ssilver, pd.merge(df_sgold, pd.merge(df_splat, df_sdol, left_index=True, right_index=True), left_index=True, right_index=True), left_index=True, right_index=True)
df_merged.to_csv('merged7.csv', index=True)
df_merged.count()

"""#**Train/Test Split**"""

# split data feature from target
FEATURES = ['Gold Price', 'Platinum Price', 'USD/EUR']
TARGET = 'Silver Price'

X = df_merged[FEATURES]
y = df_merged[TARGET]
fold = KFold(n_splits=5)
fold_5 = fold.split(X, y)

mape_scores=[]
rmse_scores=[]

for k, (train, test) in enumerate(fold_5):
  if k == 0:
        split_train = train
        split_test = test
        break
train = df_merged.iloc[split_train]
test = df_merged.iloc[split_test]
X_train = X.iloc[split_train]
y_train = y.iloc[split_train]
X_test = X.iloc[split_test]
y_test = y.iloc[split_test]

"""# **Building XGBoost Model**"""

# build initial model
model = xgb.XGBRegressor(eval_metric=['mape','rmse'])

model.fit(X_train, y_train,
        eval_set=[(X_train, y_train), (X_test, y_test)], verbose=10)

# Get predictions on the test set
y_pred = model.predict(X_test)

# Create a DataFrame with the predictions
predictions_df = pd.DataFrame(y_pred, columns=['predictions'])

# Save predictions to a CSV file
predictions_df.to_csv('predictions.csv', index=False)

# plot initial forecast
test['forecast'] = model.predict(X_test)
df = df_merged.merge(test[['forecast']], how='left', left_index=True, right_index=True)
ax = df[['Silver Price']].plot(figsize=(13, 5))
df['forecast'].plot(ax=ax, style='-')
plt.legend(['Harga perak aktual', 'Hasil peramalan'])

ax.set_title('Harga perak aktual and Hasil peramalan')
plt.show()

# plot initial forecast
test['forecast'] = model.predict(X_test)
df = df_merged.merge(test[['forecast']], how='left', left_index=True, right_index=True)
ax = df[['Silver Price']].plot(figsize=(13, 5))
df['forecast'].plot(ax=ax, style='-')
plt.legend(['Actual silver price', 'Forecasting results'])

ax.set_title('Actual silver price and Forecasting results')
plt.show()

# Get the integer indices for the test set
indices = range(len(X.iloc[split_test]))

# Plotting the forecast results
plt.figure(figsize=(10, 6))
plt.plot(indices, y.iloc[split_test], label='Harga Perak Aktual')
plt.plot(indices, model.predict(X.iloc[split_test]), label='Peramalan Harga Perak')
plt.xlabel('Indeks')
plt.ylabel('Dollar')
plt.title('Perbandingan data aktual dan peramalan harga perak')
plt.legend()
plt.grid(True)
plt.show()

# visualize each tree from initial model
fig, ax = plt.subplots(figsize=(30, 30))
xgb.plot_tree(model, num_trees=99, rankdir='LR', ax=ax)
plt.show()

"""# **Hyperparameter Tuning**

## max_depth
"""

# Define the range of values for the hyperparameter
hyperparameter_values = [2,3,4,5,6,7,8,9,10]

# Initialize lists to store hyperparameter values and corresponding results
hyperparameter_results = []
hyperparameter_labels = []
rmse_results = []
rmse_labels = []

# Perform XGBoost training with different hyperparameter values
for value in hyperparameter_values:
    # Create an instance of XGBoost regressor with the specific hyperparameter value
    model = XGBRegressor(eval_metric=['mape','rmse'], max_depth=value)

    # Train the model (assuming you have your training data X_train and corresponding labels y_train)
    model.fit(X_train, y_train,
        eval_set=[(X_train, y_train), (X_test, y_test)],
        verbose=0)
    eval_results = model.evals_result()
    last = len(eval_results['validation_1']['mape']) - 1

    # Calculate MAPE
    mape = eval_results['validation_1']['mape'][last]

    # Store the hyperparameter value and corresponding MAPE result
    hyperparameter_results.append(mape)
    hyperparameter_labels.append(str(value))

    # Calculate RMSE
    rmse = eval_results['validation_1']['rmse'][last]

    # Store the hyperparameter value and corresponding RMSE result
    rmse_results.append(rmse)
    rmse_labels.append(str(value))

# Plot the hyperparameter values against the MAPE results
plt.rcParams['font.size'] = 15
plt.figure(figsize=(8, 5))
plt.plot(hyperparameter_labels, hyperparameter_results)
plt.xlabel('max_depth',style='italic')
plt.ylabel('MAPE')
plt.xticks(rotation=45)
plt.show()

# Plot the hyperparameter values against the RMSE results
plt.rcParams['font.size'] = 15
plt.figure(figsize=(8, 5))
plt.plot(rmse_labels, rmse_results)
plt.xlabel('max_depth',style='italic')
plt.ylabel('RMSE')
plt.xticks(rotation=45)
plt.show()

"""## gamma"""

# Define the range of values for the hyperparameter
start_value = 0
increment = 5
num_values = 50

hyperparameter_values = []
current_value = start_value

for _ in range(num_values):
    hyperparameter_values.append(current_value)
    current_value += increment

print(hyperparameter_values)

# Initialize lists to store hyperparameter values and corresponding results
hyperparameter_results = []
hyperparameter_labels = []
rmse_results = []
rmse_labels = []

# Perform XGBoost training with different hyperparameter values
for value in hyperparameter_values:
    # Create an instance of XGBoost regressor with the specific hyperparameter value
    model = XGBRegressor(eval_metric=['mape','rmse'], gamma=value)

    # Train the model (assuming you have your training data X_train and corresponding labels y_train)
    model.fit(X_train, y_train,
        eval_set=[(X_train, y_train), (X_test, y_test)],
        verbose=0)
    eval_results = model.evals_result()
    last = len(eval_results['validation_1']['mape']) - 1

    # Calculate MAPE
    mape = eval_results['validation_1']['mape'][last]

    # Store the hyperparameter value and corresponding MAPE result
    hyperparameter_results.append(mape)
    hyperparameter_labels.append(str(value))

    # Calculate RMSE
    rmse = eval_results['validation_1']['rmse'][last]

    # Store the hyperparameter value and corresponding RMSE result
    rmse_results.append(rmse)
    rmse_labels.append(str(value))

# Plot the hyperparameter values against the MAPE results
plt.rcParams['font.size'] = 11
plt.figure(figsize=(13, 10))
plt.plot(hyperparameter_labels, hyperparameter_results)
plt.xlabel('gamma',style='italic')
plt.ylabel('MAPE')
plt.xticks(rotation=45)
plt.show()

# Plot the hyperparameter values against the RMSE results
plt.rcParams['font.size'] = 11
plt.figure(figsize=(13, 10))
plt.plot(rmse_labels, rmse_results)
plt.xlabel('gamma',style='italic')
plt.ylabel('RMSE')
plt.xticks(rotation=45)
plt.show()

"""## learning_rate"""

hyperparameter_values = [0.001, 0.005, 0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.5]

# Initialize lists to store hyperparameter values and corresponding results
hyperparameter_results = []
hyperparameter_labels = []
rmse_results = []
rmse_labels = []

# Perform XGBoost training with different hyperparameter values
for value in hyperparameter_values:
    # Create an instance of XGBoost regressor with the specific hyperparameter value
    model = XGBRegressor(eval_metric=['mape','rmse'], learning_rate=value)

    # Train the model (assuming you have your training data X_train and corresponding labels y_train)
    model.fit(X_train, y_train,
        eval_set=[(X_train, y_train), (X_test, y_test)],
        verbose=0)
    eval_results = model.evals_result()
    last = len(eval_results['validation_1']['mape']) - 1

    # Calculate MAPE
    mape = eval_results['validation_1']['mape'][last]

    # Store the hyperparameter value and corresponding MAPE result
    hyperparameter_results.append(mape)
    hyperparameter_labels.append(str(value))

    # Calculate RMSE
    rmse = eval_results['validation_1']['rmse'][last]

    # Store the hyperparameter value and corresponding RMSE result
    rmse_results.append(rmse)
    rmse_labels.append(str(value))

# Plot the hyperparameter values against the MAPE results
plt.rcParams['font.size'] = 15
plt.figure(figsize=(8, 5))
plt.plot(hyperparameter_labels, hyperparameter_results)
plt.xlabel('learning_rate',style='italic')
plt.ylabel('MAPE')
plt.xticks(rotation=45)
plt.show()

# Plot the hyperparameter values against the RMSE results
plt.rcParams['font.size'] = 15
plt.figure(figsize=(8, 5))
plt.plot(rmse_labels, rmse_results)
plt.xlabel('learning_rate',style='italic')
plt.ylabel('RMSE')
plt.xticks(rotation=45)
plt.show()

"""## n_estimators"""

# Define the range of values for the hyperparameter
start_value = 10
increment = 10
num_values = 20

hyperparameter_values = []
current_value = start_value

for _ in range(num_values):
    hyperparameter_values.append(current_value)
    current_value += increment

print(hyperparameter_values)

# Initialize lists to store hyperparameter values and corresponding results
hyperparameter_results = []
hyperparameter_labels = []
rmse_results = []
rmse_labels = []

# Perform XGBoost training with different hyperparameter values
for value in hyperparameter_values:
    # Create an instance of XGBoost regressor with the specific hyperparameter value
    model = XGBRegressor(eval_metric=['mape','rmse'], n_estimators=value)

    # Train the model (assuming you have your training data X_train and corresponding labels y_train)
    model.fit(X_train, y_train,
        eval_set=[(X_train, y_train), (X_test, y_test)],
        verbose=0)
    eval_results = model.evals_result()
    last = len(eval_results['validation_1']['mape']) - 1

    # Calculate MAPE
    mape = eval_results['validation_1']['mape'][last]

    # Store the hyperparameter value and corresponding MAPE result
    hyperparameter_results.append(mape)
    hyperparameter_labels.append(str(value))

    # Calculate RMSE
    rmse = eval_results['validation_1']['rmse'][last]

    # Store the hyperparameter value and corresponding RMSE result
    rmse_results.append(rmse)
    rmse_labels.append(str(value))

# Plot the hyperparameter values against the MAPE results
plt.rcParams['font.size'] = 15
plt.figure(figsize=(15, 5))
plt.plot(hyperparameter_labels, hyperparameter_results)
plt.xlabel('n_estimators',style='italic')
plt.ylabel('MAPE')
plt.xticks(rotation=45)
plt.show()

# Plot the hyperparameter values against the RMSE results
plt.rcParams['font.size'] = 15
plt.figure(figsize=(15, 5))
plt.plot(rmse_labels, rmse_results)
plt.xlabel('n_estimators',style='italic')
plt.ylabel('RMSE')
plt.xticks(rotation=45)
plt.show()

"""## ALL Hyperparameter"""

# define hyperparameter combination
hyperparameters = {
    'learning_rate': [0.05, 0.1, 0.15],
    'max_depth': [2, 3, 4, 6],
    'n_estimators': [20, 100, 130],
    'gamma': [0, 45, 70]
}

results_table = pd.DataFrame(columns=['Learning_Rate', 'Max_Depth', 'N_Estimators', 'Gamma', 'RMSE_Score'])

from itertools import product

# Create a list to store the attempts and RMSE scores
results = []

for params in product(*hyperparameters.values()):
    # Create and fit the model with the current hyperparameters
    learning_rate, max_depth, n_estimators, gamma = params
    model = xgb.XGBRegressor(
        eval_metric=['mape','rmse'],
        learning_rate=learning_rate,
        max_depth=max_depth,
        n_estimators=n_estimators,
        gamma=gamma
    )
    model.fit(X.iloc[split_train], y.iloc[split_train],
        eval_set=[(X.iloc[split_train], y.iloc[split_train]), (X.iloc[split_test], y.iloc[split_test])], verbose=0)
    eval_results = model.evals_result()
    last = len(eval_results['validation_1']['mape']) - 1
    mape = eval_results['validation_1']['mape'][last]
    rmse = eval_results['validation_1']['rmse'][last]

    # Store the results
    result = {
        'Learning_Rate': learning_rate,
        'Max_Depth': max_depth,
        'N_Estimators': n_estimators,
        'Gamma': gamma,
        'RMSE_Score': rmse,
        'MAPE_Score': mape
    }
    results.append(result)

# Create a DataFrame from the results list
results_table = pd.DataFrame(results)

# Save the results table to a CSV file
results_table.to_excel('hyperparameter_results.xlsx')

# Print the results table
results_table

"""# **Forecasting (Model A)**"""

# slicing data for forecasting
slice_date_from = '2023-02-21'
slice_date_to = '2023-02-28'

f_silver = df_silver[slice_date_from : slice_date_to]
f_gold = df_gold[slice_date_from : slice_date_to]
f_plat = df_plat[slice_date_from : slice_date_to]
f_dol = df_dol[slice_date_from : slice_date_to]

# merge data into one dataframe
df_merged_f = pd.merge(f_gold, pd.merge(f_plat, f_dol, left_index=True, right_index=True), left_index=True, right_index=True)
df_merged_f

# Best Model (A)
model = xgb.XGBRegressor(eval_metric=['mape','rmse'], n_estimators=130, max_depth=2, learning_rate=0.15)

model.fit(X.iloc[split_train], y.iloc[split_train],
        eval_set=[(X.iloc[split_train], y.iloc[split_train]), (X.iloc[split_test], y.iloc[split_test])], verbose=10)

X = df_merged[FEATURES]
y = df_merged[TARGET]
fold = KFold(n_splits=5)
fold_5 = fold.split(X, y)

mape_scores=[]
rmse_scores=[]

for k, (train_set, test_set) in enumerate(fold_5):
  model.fit(X.iloc[train_set], y.iloc[train_set],
        eval_set=[(X.iloc[train_set], y.iloc[train_set]), (X.iloc[test_set], y.iloc[test_set])], verbose=0)
  eval_results = model.evals_result()
  last = len(eval_results['validation_1']['mape']) - 1
  mape_scores.append(eval_results['validation_1']['mape'][last])
  rmse_scores.append(eval_results['validation_1']['rmse'][last])

print(mape_scores)
print(rmse_scores)

# plot initial forecast
test['forecast'] = model.predict(X_test)
df = df_merged.merge(test[['forecast']], how='left', left_index=True, right_index=True)
ax = df[['Silver Price']].plot(figsize=(13, 5))
df['forecast'].plot(ax=ax, style='-')
plt.legend(['Harga perak aktual', 'Hasil peramalan'])

ax.set_title('Harga perak aktual and Hasil peramalan')
plt.show()

# Make predictions on the test set
predictions = model.predict(df_merged_f)

# Print the predictions
print(predictions)

"""#**Forecasting (Model B)**"""

# Best Model (B)
model = xgb.XGBRegressor(eval_metric=['mape','rmse'], n_estimators=130, max_depth=3, learning_rate=0.1)

model.fit(X.iloc[split_train], y.iloc[split_train],
        eval_set=[(X.iloc[split_train], y.iloc[split_train]), (X.iloc[split_test], y.iloc[split_test])], verbose=10)

# Get predictions on the test set
y_pred = model.predict(X.iloc[split_test])

# Create a DataFrame with the predictions
predictions_df = pd.DataFrame(y_pred, columns=['predictions'])

# Save predictions to a CSV file
predictions_df.to_csv('predictions.csv', index=False)

X = df_merged[FEATURES]
y = df_merged[TARGET]
fold = KFold(n_splits=5)
fold_5 = fold.split(X, y)

mape_scores=[]
rmse_scores=[]

for k, (train_set, test_set) in enumerate(fold_5):
  model.fit(X.iloc[train_set], y.iloc[train_set],
        eval_set=[(X.iloc[train_set], y.iloc[train_set]), (X.iloc[test_set], y.iloc[test_set])], verbose=0)
  eval_results = model.evals_result()
  last = len(eval_results['validation_1']['mape']) - 1
  mape_scores.append(eval_results['validation_1']['mape'][last])
  rmse_scores.append(eval_results['validation_1']['rmse'][last])

print(mape_scores)
print(rmse_scores)

# plot initial forecast
test['forecast2'] = model.predict(X_test)
df = df_merged.merge(test[['forecast2']], how='left', left_index=True, right_index=True)
ax = df[['Silver Price']].plot(figsize=(13, 5))
df['forecast2'].plot(ax=ax, style='-')
plt.legend(['Harga perak aktual', 'Hasil peramalan'])

ax.set_title('Harga perak aktual and Hasil peramalan')
plt.show()

# Make predictions on the test set
predictions = model.predict(df_merged_f)

# Print the predictions
print(predictions)

